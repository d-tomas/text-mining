{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lecture_3.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOqwodrx8aUjRdq99u5uPwF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d-tomas/text-mining/blob/main/notebooks/lecture_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1AwPWOa4LTA"
      },
      "source": [
        "# **Day 3**: Supervised Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vftAQFLq5Qa8"
      },
      "source": [
        "## Initial setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bXm4awKjMGU"
      },
      "source": [
        "# Import the required libraries\n",
        "\n",
        "import matplotlib.pyplot as plt  # Para hacer gráficas\n",
        "import numpy as np  # Obtener valores únicos en un vector\n",
        "import pandas as pd\n",
        "import seaborn as sns  # Visualización del mapa de calor\n",
        "from sklearn.metrics import accuracy_score  # Calcular la precisión del clasificador\n",
        "from sklearn.model_selection import train_test_split  # Separar el dataset en entrenamiento y test\n",
        "from sklearn.metrics import confusion_matrix  # Sacar la matriz de confusión\n",
        "from sklearn.metrics import mean_absolute_error  # Mean Absolut Error (MAE) para regresión\n",
        "from sklearn.svm import SVC  # Algoritmo Support Vector Machines\n",
        "from sklearn.tree import DecisionTreeClassifier  # Decission tree algorithm\n",
        "from sklearn.naive_bayes import MultinomialNB  # Naïve Bayes\n",
        "from sklearn.neural_network import MLPClassifier  # Neural Networks\n",
        "from sklearn.neighbors import KNeighborsClassifier  # k-NN\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # Matriz de términos por documento con TF-IDF\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import cross_val_score  # Cross-validation evaluation\n",
        "import spacy  # NLP library\n",
        "\n",
        "# Install the SpaCy model for English texts\n",
        "spacy.cli.download('en_core_web_sm')\n",
        "\n",
        "# Load the model\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner', 'entity_likner', 'entity_ruler'])\n",
        "\n",
        "# Descargamos el corpus para entrenar y evaluar el sistema de regresión\n",
        "!wget https://raw.githubusercontent.com/d-tomas/text-mining/main/datasets/repec_s.csv\n",
        "# Descargamos el corpus para entrenar y evaluar el sistema de clasificación\n",
        "!wget https://raw.githubusercontent.com/d-tomas/text-mining/main/datasets/cell_phones.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRO0rlVXlkKW"
      },
      "source": [
        "## Example 1: text classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mqNDDvWlmhw"
      },
      "source": [
        "# Vemos que pinta tiene el corpus de entrenamiento\n",
        "\n",
        "!head repec_s.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "et2IrRJsmtIP"
      },
      "source": [
        "# Loading data from file\n",
        "\n",
        "data = pd.read_csv('repec_s.csv')  # Cargamos los datos del fichero\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlgNyttpnIR4"
      },
      "source": [
        "# Classify based on the abstracts\n",
        "\n",
        "corpus = data['abstract']  # Store the abstracts\n",
        "y = data['jel']  # Store the JEL category"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HELdN7HBm5H_"
      },
      "source": [
        "# Plot the classes\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.countplot(x=y)\n",
        "plt.show()\n",
        "\n",
        "# F:\tInternational Economics\n",
        "# I:\tHealth, Education, and Welfare\n",
        "# R:\tUrban, Rural, Regional, Real Estate, and Transportation Economics\n",
        "# M:\tBusiness Administration and Business Economics | Marketing | Accounting | Personnel Economics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXreFadJo9B8"
      },
      "source": [
        "# Preprocessing\n",
        "# For each abstract: remove punctuation, remove stopwords, and lowercase\n",
        "#corpus_normalised = list(nlp.pipe(corpus.values, disable=['parser', 'ner', 'entity_likner', 'entity_ruler']))\n",
        "\n",
        "def normalise(text):\n",
        "  document = nlp(text)  # Process the text with SpaCy\n",
        "  document = [token for token in document if not token.is_punct]  # Remove punctuation\n",
        "  document = [token for token in document if not token.is_stop]  # Remove stopwords\n",
        "  document = [token.lower_ for token in document]  # Lowercase\n",
        "  return ' '.join(document)\n",
        "\n",
        "corpus_normalised = corpus.map(normalise)\n",
        "corpus_normalised"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgtWyIEFm4b_"
      },
      "source": [
        "# Tenemos que transformar las palabras en números\n",
        "# Cada palabra del mensaje se representa por su TF-IDF\n",
        "\n",
        "def classify(corpus, model_name, evaluation_type):\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "  if model_name == 'DT':\n",
        "    model = DecisionTreeClassifier()  # Decission tree\n",
        "  elif model_name == 'KNN':\n",
        "    model = KNeighborsClassifier()  # k-NN\n",
        "  elif model_name == 'MLP':  \n",
        "    model = MLPClassifier()  # Neural network\n",
        "  elif model_name == 'NB':\n",
        "    model = MultinomialNB()  # Naïve Bayes\n",
        "  else:\n",
        "    model = SVC(kernel = 'linear')  # SVM\n",
        "\n",
        "  if evaluation_type == 'split':\n",
        "    # Separamos el corpus en entrenamiento (80%) y test (20%)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "    \n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Prediction over the test set\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    # Calculate the accuracy of the algorithm\n",
        "    print('Accuracy: {:.2%}\\n'.format(accuracy_score(predictions, y_test)))\n",
        "    print('Confusion matrix:')\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(confusion_matrix(y_test, predictions), annot=True, linewidth=3)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.show()\n",
        "  elif evaluation_type == 'cv':\n",
        "    scores = cross_val_score(model, X, y, cv=5)\n",
        "    print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
        "  else:\n",
        "    print('Unknown evaluation type')\n",
        "  \n",
        "  return model, vectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC_Q2NUmwC6J"
      },
      "source": [
        "# Possible values for model name: DT, KNN, MLP, NB, SVM\r\n",
        "# Possible values for evaluation type: split, cv\r\n",
        "# Returns the trained model, for further predictions\r\n",
        "\r\n",
        "model_repec, vectorizer_repec = classify(corpus, 'SVM', 'split')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4J1cKzL9sGdp"
      },
      "source": [
        "# Prediction\r\n",
        "\r\n",
        "new_input = ['My car is a Ferrari']\r\n",
        "# Tenemos que transformar el texto a números, como se hizo al entrenar\r\n",
        "new_input = vectorizer_repec.transform(new_input)\r\n",
        "label = model_repec.predict(new_input)  # Predecimos la etiqueta para la nueva entrada (POS o NEG)\r\n",
        "\r\n",
        "if label == 'F':\r\n",
        "  print('International Economics')\r\n",
        "elif label == 'I':\r\n",
        "  print('Health, Education, and Welfare')\r\n",
        "elif label == 'R':\r\n",
        "  print('Urban, Rural, Regional, Real Estate, and Transportation Economics')\r\n",
        "elif label == 'M':\r\n",
        "  print('Business Administration and Business Economics | Marketing | Accounting | Personnel Economics')\r\n",
        "else:\r\n",
        "  print('Unknown class')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjJfjcStqsBA"
      },
      "source": [
        "### Exercise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZIwuhk8qtk8"
      },
      "source": [
        "# Test with titles instead of abstracts\r\n",
        "# Tip: corpus = data['title']\r\n",
        "\r\n",
        "# Try using different n-gram sizes\r\n",
        "# Tip: vectorizer = TfidfVectorizer(ngram_range=(1,2))  # Uses 1-grams and 2-grams\r\n",
        "\r\n",
        "# Try using TF weighting schema\r\n",
        "# Tip: vectorizer = CountVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5G1GRsddqMEa"
      },
      "source": [
        "## Example 2: sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lt3xZe9cuksC"
      },
      "source": [
        "# Vemos que pinta tiene el corpus de entrenamiento\n",
        "\n",
        "!head cell_phones.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0mrXTXMuoyI"
      },
      "source": [
        "# Loading data from file\n",
        "\n",
        "data = pd.read_csv('cell_phones.csv')  # Cargamos los datos del fichero\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFg9jkd2uyrO"
      },
      "source": [
        "# Extract the comments and labels\n",
        "\n",
        "corpus = data['content']  # Store the comments\n",
        "y = data['opinion']  # Store positive or negative labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PLd3TOFuyrP"
      },
      "source": [
        "# Plot the classes\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.countplot(x=y)\n",
        "plt.show()\n",
        "\n",
        "# POS: positive opinion\n",
        "# NEG: negative opinion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3wtEzENuyrR"
      },
      "source": [
        "# Preprocessing\n",
        "# Re-use the 'normalise' function\n",
        "\n",
        "corpus_normalised = corpus.map(normalise)\n",
        "corpus_normalised"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yGJDtwyuyrU"
      },
      "source": [
        "# Use the 'classify' function as before\n",
        "# Possible values for model name: DT, KNN, MLP, NB, SVM\n",
        "# Possible values for evaluation type: split, cv\n",
        "\n",
        "model_phones, vectorizer_phones = classify(corpus, 'SVM', 'split')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tBUi_DpuyrV"
      },
      "source": [
        "# Prediction\r\n",
        "\r\n",
        "new_input = ['I love this phone!!']\r\n",
        "# Tenemos que transformar el texto a números, como se hizo al entrenar\r\n",
        "new_input = vectorizer_phones.transform(new_input)\r\n",
        "label = model_phones.predict(new_input)  # Predecimos la etiqueta para la nueva entrada (POS o NEG)\r\n",
        "\r\n",
        "if label == 'POS':\r\n",
        "  print('Positive opinion')\r\n",
        "elif label == 'NEG':\r\n",
        "  print('Negative opinion')\r\n",
        "else:\r\n",
        "  print('Unknown class')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX3Y3HRY110Z"
      },
      "source": [
        "### Exercise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T58yyl0v180n"
      },
      "source": [
        "# Create a wordcloud of positive opinions\r\n",
        "# Tip: data = data[data['opinion'] == 'POS']\r\n",
        "# Tip: from wordcloud import WordCloud  # Required library"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2xOmNsq3YGk"
      },
      "source": [
        "# Create a wordcloud of negative opinions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuGoggVzTv4_"
      },
      "source": [
        "# References\n",
        "\n",
        "* [RePEC](http://www.repec.org/)\n",
        "* [JEL Classification System](https://www.aeaweb.org/econlit/jelCodes.php?view=jel)\n"
      ]
    }
  ]
}