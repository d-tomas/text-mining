{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lecture_01.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOy2NNBkM8iNKKprYkFiQ8t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d-tomas/text-mining/blob/main/notebooks/lecture_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1AwPWOa4LTA"
      },
      "source": [
        "# **Day 1**: Text Mining Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vftAQFLq5Qa8"
      },
      "source": [
        "## Initial setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSoUiL_W4Eg5"
      },
      "source": [
        "# Import the required libraries\n",
        "\n",
        "import spacy  # NLP library\n",
        "import pandas as pd  # Table manipulation\n",
        "import matplotlib.pyplot as plt  # Visualisation\n",
        "import seaborn as sns  # Visualisation\n",
        "import nltk  # NLP library\n",
        "from nltk.corpus import wordnet  # WordNet\n",
        "\n",
        "# Install the SpaCy model for English texts\n",
        "spacy.cli.download('en_core_web_sm')\n",
        "\n",
        "# Download WordNet\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Download example text file ('news.txt')\n",
        "!wget https://raw.githubusercontent.com/d-tomas/text-mining/main/datasets/news.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_YOZet95b0m"
      },
      "source": [
        "## Example 1: part-of-speech tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOzyeno7fddD"
      },
      "source": [
        "# Process and annotate text with the SpaCy model\n",
        "\n",
        "text = 'Today is Thursday, February 4, 2021. It is 3:00 p.m. I am attending a Text Mining seminar at the University of Alicante, in Spain. The teacher is David. He tries to make it interesting but sometimes fails.'\n",
        "document = nlp(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXSI__07fmZz"
      },
      "source": [
        "# Extract the list of sentences from text\n",
        "\n",
        "list(document.sents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abl7bLud5hK2"
      },
      "source": [
        "# Extract morphological information (POS-tagging) for each word in text\n",
        "\n",
        "for token in document:  # For each token (word) in the document\n",
        "    print('Word: ' + token.text)\n",
        "    print('Lemma: ' + token.lemma_)\n",
        "    print('POS: ' + token.pos_)\n",
        "    print('POS fine: ' + token.tag_)\n",
        "    print('---')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfPvMReXedYw"
      },
      "source": [
        "# You can use 'explain' if you do not understand the meaning of a POS tag\n",
        "\n",
        "spacy.explain('VBZ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COVPXVTnV5YF"
      },
      "source": [
        "# Create a DataFrame based on the content for further analysis\n",
        "\n",
        "df = pd.DataFrame(data=[[token.text, token.lemma_, token.pos_, token.tag_] for token in document], columns=['Word', 'Lemma', 'POS', 'POS fine'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8ZAGLDzY2im"
      },
      "source": [
        "# Basic statistics of the columns\n",
        "\n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFGlCZXkY9Pn"
      },
      "source": [
        "# What is the number of verbs in the text?\n",
        "\n",
        "(df['POS'] == 'VERB').sum()  # Substitute 'VERB' with any other POS tag (e.g. 'PUNCT')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxHivGQjX516"
      },
      "source": [
        "# we can do some interesting visualisations\n",
        "# Bar plot with the count of each POS tag\n",
        "\n",
        "plt.figure(figsize=(14,7))\n",
        "sns.countplot(x='POS', data=df)\n",
        "plt.xticks(rotation=-45)  # Rotamos las etiquetas para que no se solapen\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7cC9UAb_-bs"
      },
      "source": [
        "### Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFbY0N1xaDZA"
      },
      "source": [
        "* Do the POS-tagging of the content in the file 'news.txt'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocChesdh_7s2"
      },
      "source": [
        "# Store all the content of the file 'news.txt' in the variable\n",
        "with open('news.txt') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Insert your code below\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFqwX8p0aHrW"
      },
      "source": [
        "* How many adjectives are there in the text?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s6L7QbsainW"
      },
      "source": [
        "# Insert your code below\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQt_nNnfDZLi"
      },
      "source": [
        "## Example 2: shallow parsing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFwX-dlrDbQd"
      },
      "source": [
        "# Get all the noun phrases from text\n",
        "\n",
        "for chunk in document.noun_chunks:\n",
        "    print('Noun phrase: ' + chunk.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uCmyakyD4qo"
      },
      "source": [
        "# 'displacy' shows the parse tree\n",
        "\n",
        "spacy.displacy.render(document, style = 'dep', options = {'compact': True}, jupyter = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMCnuMABD7xI"
      },
      "source": [
        "# Navigate the dependency tree\n",
        "# - 'head' and 'child' describe words connected in the dependency tree\n",
        "# - 'dep' is the type of syntactic relation connecting 'child' and 'head'\n",
        "\n",
        "for token in document:\n",
        "    print(token.text, token.dep_, token.head.text, token.head.pos_, [child for child in token.children])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo-lGalAUgZN"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Complete the code in the following cell to (shallow) parse the content in the file 'news.txt'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGsn0SXwUko2"
      },
      "source": [
        "# Insert your code below"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEQaa0E6_smk"
      },
      "source": [
        "## Example 3: semantic analysis with WordNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8-b-4sxAwKF"
      },
      "source": [
        "# Get all the synsets of a word\n",
        "\n",
        "word = 'dog'\n",
        "\n",
        "list_synsets = wordnet.synsets(word)\n",
        "for synset in list_synsets:\n",
        "  print('Synset: ' + synset.name())\n",
        "  print('Lemma: ' + synset.lemmas()[0].name())\n",
        "  print('Meaning: ' + synset.definition())\n",
        "  print('Examples: ' + str(synset.examples()))\n",
        "  print('---')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwUnezNoCt1l"
      },
      "source": [
        "# Get synonyms and antonyms\n",
        "\n",
        "word = 'tall'\n",
        "\n",
        "list_synsets = wordnet.synsets(word)\n",
        "list_sinonyms = set()  # Use 'set' instead of 'list' to avoid duplicates\n",
        "list_antonyms = set()\n",
        "for synset in list_synsets:\n",
        "  for lemma in synset.lemmas():\n",
        "    list_sinonyms.add(lemma.name())\n",
        "    if lemma.antonyms():\n",
        "      list_antonyms.add(lemma.antonyms()[0].name())\n",
        "\n",
        "print('Synonyms: ' + str(list_sinonyms))\n",
        "print('Antonyms: ' + str(list_antonyms))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7GT83mZacX3"
      },
      "source": [
        "# Get all the hypernyms\n",
        "\n",
        "word = 'terrier'\n",
        "\n",
        "synset = wordnet.synsets(word)[0]  # First synset of the word\n",
        "hypernyms = lambda s:s.hypernyms()\n",
        "\n",
        "print(list(synset.closure(hypernyms)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
