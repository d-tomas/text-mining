{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lecture_2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMxwV3gOyNgBLKR6wh9Mp//",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d-tomas/text-mining/blob/main/notebooks/lecture_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1AwPWOa4LTA"
      },
      "source": [
        "# **Day 2**: Document Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vftAQFLq5Qa8"
      },
      "source": [
        "## Initial setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSoUiL_W4Eg5"
      },
      "source": [
        "# Import the required libraries\n",
        "\n",
        "import nltk  # NLP library\n",
        "from nltk.stem.porter import *  # Stemmer tool\n",
        "from sklearn.feature_extraction.text import CountVectorizer  # Term by document matrix with TF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # Term by document matrix with TF-IDF\n",
        "import spacy  # NLP library\n",
        "\n",
        "# Install the SpaCy model for English texts\n",
        "spacy.cli.download('en_core_web_sm')\n",
        "\n",
        "# Load the model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Download example text files ('news.txt' and 'paper.txt')\n",
        "!wget https://raw.githubusercontent.com/d-tomas/text-mining/main/datasets/news.txt\n",
        "!wget https://raw.githubusercontent.com/d-tomas/text-mining/main/datasets/paper.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmCPGSE1RjBj"
      },
      "source": [
        "## Example 1: n-gram extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTe_q_4VUZyk"
      },
      "source": [
        "# Extract bigrams and trigrams from text\n",
        "\n",
        "with open('news.txt') as file:\n",
        "    content = file.read()\n",
        "\n",
        "list_bigrams = nltk.ngrams(content.split(), 2)  # split() the sentence into a list of words\n",
        "list_trigrams = nltk.ngrams(content.split(), 3)\n",
        "\n",
        "print('---------')\n",
        "print('Bigrams:')\n",
        "print('---------')\n",
        "for bigram in list_bigrams:\n",
        "  print(bigram)\n",
        "\n",
        "print('----------')\n",
        "print('Trigrams:')\n",
        "print('----------')\n",
        "for trigram in list_trigrams:\n",
        "  print(trigram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_NRVKsGSQb6"
      },
      "source": [
        "# The previous approach does not consider sentence boundaries\n",
        "# We can read the file line by line and extract n-grams for each line separately\n",
        "\n",
        "with open('news.txt') as file:\n",
        "    content = file.readlines()  # Get a list of lines\n",
        "\n",
        "# Remove empty lines, blanks and new line characters\n",
        "content = [line.strip() for line in content if line.strip()]\n",
        "\n",
        "for line in content:\n",
        "    trigrams = nltk.ngrams(line.split(), 3)  # Extract 3-grams for each line\n",
        "    for trigram in trigrams:\n",
        "        print(trigram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxkaGPOSVtk6"
      },
      "source": [
        "### Exercise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoxeREt_VYjd"
      },
      "source": [
        "# Repeat the analysis on 'paper.txt', obtaining also 4-grams and 5-grams in addition to bigrams and trigrams\n",
        "# Use the first procedure (no need to consider sentence boundaries)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aDi5mm9aj3R"
      },
      "source": [
        "## Example 2: normalisation / pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzG05ZncXWcK"
      },
      "source": [
        "# Remove punctuation, lowercase, remove stopwords and get the stem of the words\n",
        "\n",
        "text = 'The Netherlands earned sweet revenge on Spain on Friday at the Fonte Nova in Salvador, hammering Spain 5-1 to put an emphatic coda on their loss in the 2010 World Cup finals.'\n",
        "\n",
        "document = nlp(text)  # Process the text with SpaCy\n",
        "\n",
        "document = [token for token in document if not token.is_punct]  # Remove punctuation\n",
        "print('No punctuation: ' + str(document))\n",
        "\n",
        "document = [token for token in document if not token.is_stop]  # Remove stopwords\n",
        "print('No stopwords: ' + str(document))\n",
        "\n",
        "document = [token.lower_ for token in document]  # Lowercase\n",
        "print('Lowercased: ' + str(document))\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "document = [stemmer.stem(token) for token in document]  # Stem of the words\n",
        "print('Stems: ' + str(document))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5gASp9Me70T"
      },
      "source": [
        "### Exercise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqDpnBNoe7Ki"
      },
      "source": [
        "# Repeat the previous analysis on the content of 'paper.txt'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmOJ7nl1dwCi"
      },
      "source": [
        "## Example 3: weighting schema"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5bTyix3dyf2"
      },
      "source": [
        "# Build the term by document matrix using the TF weighting schema\n",
        "\n",
        "corpus = ['I do not like this restaurant', 'I like this restaurant very much', 'I think it is a very very bad place', 'I love this place']\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names())\n",
        "print(X.toarray())\n",
        "print(X.shape)\n",
        "\n",
        "vectorizer2 = CountVectorizer(analyzer = 'word', ngram_range = (2, 2))  # Extract bigrams\n",
        "X2 = vectorizer2.fit_transform(corpus)\n",
        "print(vectorizer2.get_feature_names())\n",
        "print(X2.toarray())\n",
        "print(X2.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uORvGBOGet-j"
      },
      "source": [
        "# Build the term by document matrix using the TF-IDF weighting schema\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names())\n",
        "print(X.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xbsFpsjhr4l"
      },
      "source": [
        "### Exercise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FRsB7FFhtOs"
      },
      "source": [
        "# Get the term by document matrix, using TF weighting schema and trigrams on 'news.txt'"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}