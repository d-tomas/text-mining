{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lecture_2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO8Zsmz+Z9SPUlkXns3djTX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d-tomas/text-mining/blob/main/notebooks/lecture_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1AwPWOa4LTA"
      },
      "source": [
        "# **Day 2**: Document Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vftAQFLq5Qa8"
      },
      "source": [
        "## Initial setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSoUiL_W4Eg5"
      },
      "source": [
        "# Import the required libraries\n",
        "\n",
        "import gensim  # Word embedding models\n",
        "from gensim.models import KeyedVectors  # Load pre-trained word embedding models\n",
        "import matplotlib.pyplot as plt  # Display word clouds\n",
        "import nltk  # NLP library\n",
        "from nltk.stem.porter import *  # Stemmer tool\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer  # Term by document matrix with TF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # Term by document matrix with TF-IDF\n",
        "import spacy  # NLP library\n",
        "from wordcloud import WordCloud  # Create word clouds\n",
        "\n",
        "# Install the SpaCy model for English texts\n",
        "spacy.cli.download('en_core_web_sm')\n",
        "\n",
        "# Load the model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Download example text files ('news.txt', 'paper.txt' and 'repec_s.csv')\n",
        "!wget https://raw.githubusercontent.com/d-tomas/text-mining/main/datasets/news.txt\n",
        "!wget https://raw.githubusercontent.com/d-tomas/text-mining/main/datasets/paper.txt\n",
        "!wget https://raw.githubusercontent.com/d-tomas/text-mining/main/datasets/repec_s.csv\n",
        "\n",
        "# Download a pre-trained word embedding model with 100 billion words from Google News\n",
        "!wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
        "!gunzip GoogleNews-vectors-negative300.bin.gz  # Unzip the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmCPGSE1RjBj"
      },
      "source": [
        "## Example 1: n-gram extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTe_q_4VUZyk"
      },
      "source": [
        "# Extract bigrams and trigrams from text\n",
        "\n",
        "with open('news.txt') as file:\n",
        "    content = file.read()\n",
        "\n",
        "list_bigrams = nltk.ngrams(content.split(), 2)  # split() the sentence into a list of words\n",
        "list_trigrams = nltk.ngrams(content.split(), 3)\n",
        "\n",
        "print('---------')\n",
        "print('Bigrams:')\n",
        "print('---------')\n",
        "for bigram in list_bigrams:\n",
        "  print(bigram)\n",
        "\n",
        "print('----------')\n",
        "print('Trigrams:')\n",
        "print('----------')\n",
        "for trigram in list_trigrams:\n",
        "  print(trigram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_NRVKsGSQb6"
      },
      "source": [
        "# The previous approach does not consider sentence boundaries\n",
        "# We can read the file line by line and extract n-grams for each line separately\n",
        "\n",
        "with open('news.txt') as file:\n",
        "    content = file.readlines()  # Get a list of lines\n",
        "\n",
        "# Remove empty lines, blanks and new line characters\n",
        "content = [line.strip() for line in content if line.strip()]\n",
        "\n",
        "for line in content:\n",
        "    trigrams = nltk.ngrams(line.split(), 3)  # Extract 3-grams for each line\n",
        "    for trigram in trigrams:\n",
        "        print(trigram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxkaGPOSVtk6"
      },
      "source": [
        "### Exercise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoxeREt_VYjd"
      },
      "source": [
        "# Repeat the analysis on 'paper.txt', obtaining also 4-grams and 5-grams in addition to bigrams and trigrams\n",
        "# Use the first procedure (no need to consider sentence boundaries)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aDi5mm9aj3R"
      },
      "source": [
        "## Example 2: normalisation / pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzG05ZncXWcK"
      },
      "source": [
        "# Remove punctuation, lowercase, remove stopwords and get the stem of the words\n",
        "\n",
        "text = 'The Netherlands earned sweet revenge on Spain on Friday at the Fonte Nova in Salvador, hammering Spain 5-1 to put an emphatic coda on their loss in the 2010 World Cup finals.'\n",
        "\n",
        "document = nlp(text)  # Process the text with SpaCy\n",
        "\n",
        "document = [token for token in document if not token.is_punct]  # Remove punctuation\n",
        "print('No punctuation: ' + str(document))\n",
        "\n",
        "document = [token for token in document if not token.is_stop]  # Remove stopwords\n",
        "print('No stopwords: ' + str(document))\n",
        "\n",
        "document = [token.lower_ for token in document]  # Lowercase\n",
        "print('Lowercased: ' + str(document))\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "document = [stemmer.stem(token) for token in document]  # Stem of the words\n",
        "print('Stems: ' + str(document))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5gASp9Me70T"
      },
      "source": [
        "### Exercise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqDpnBNoe7Ki"
      },
      "source": [
        "# Repeat the previous analysis on the content of 'paper.txt'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmOJ7nl1dwCi"
      },
      "source": [
        "## Example 3: weighting schema"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5bTyix3dyf2"
      },
      "source": [
        "# Build the term by document matrix using the TF weighting schema\n",
        "\n",
        "corpus = ['I do not like this restaurant', 'I like this restaurant very much', 'I think it is a very very bad place', 'I love this place']\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names())\n",
        "print(X.toarray())\n",
        "print(X.shape)\n",
        "\n",
        "vectorizer2 = CountVectorizer(analyzer = 'word', ngram_range = (2, 2))  # Extract bigrams\n",
        "X2 = vectorizer2.fit_transform(corpus)\n",
        "print(vectorizer2.get_feature_names())\n",
        "print(X2.toarray())\n",
        "print(X2.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uORvGBOGet-j"
      },
      "source": [
        "# Build the term by document matrix using the TF-IDF weighting schema\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names())\n",
        "print(X.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xbsFpsjhr4l"
      },
      "source": [
        "### Exercise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FRsB7FFhtOs"
      },
      "source": [
        "# Get the term by document matrix, using TF weighting schema and trigrams on 'news.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMubljgLGzPm"
      },
      "source": [
        "## Example 4: word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxpYV-F6Kli7"
      },
      "source": [
        "# Load the model into memory\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A08x0V62MyAB"
      },
      "source": [
        "# Show the vector representing a word\n",
        "\n",
        "model['dog']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8QwhlbDfq0G"
      },
      "source": [
        "# Check the size of the returned vector\n",
        "\n",
        "len(model['dog'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpfnvoUQM0qx"
      },
      "source": [
        "# Get the 5 most similar words to a given one \n",
        "\n",
        "model.most_similar('monkey', topn = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8LF3h5bAr7V"
      },
      "source": [
        "# Analogy: 'France' is to 'Paris' as 'Madrid' is to... (France - Paris + Madrid)\n",
        "\n",
        "model.most_similar(positive=['Madrid', 'France'], negative=['Paris'], topn=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg2WJxX8AtwY"
      },
      "source": [
        "# Ditch unrelated terms\n",
        "\n",
        "model.doesnt_match(['Wine', 'Beer', 'Coke', 'Whysky'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dc5NtJaaAvK5"
      },
      "source": [
        "# Similarity between words\n",
        "# Beware of algorithmic bias!!\n",
        "\n",
        "model.similarity('woman', 'housework')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3fxU8syJmrr"
      },
      "source": [
        "## Example 5: word cloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVXalvGUJqG5"
      },
      "source": [
        "# The file 'repec_s.csv' contains 5,000 references to scientific papers in the field of Economics\n",
        "\n",
        "data = pd.read_csv('repec_s.csv')  # Store the data in a DataFrame\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDh2-QFyLHLl"
      },
      "source": [
        "# Create a word cloud with the titles\n",
        "\n",
        "list_lines = data['title']  # Store all the lines of the titles\n",
        "\n",
        "corpus = nlp(' '.join(list_lines))  # Concatenate all the sentences in one string\n",
        "tokens = [w.lower_ for w in corpus if (not w.is_space and not w.is_punct and not w.is_stop)]  # Lowercase removing blanks, punctuation and stopwords\n",
        "corpus = ' '.join(tokens)  # Join again all the words in one string\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=800, background_color='white', min_font_size=10).generate(corpus)  # Create word cloud\n",
        "\n",
        "# Configuration of the word cloud display\n",
        "plt.figure(figsize=(10, 10), facecolor=None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off') \n",
        "plt.tight_layout(pad=0) \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8cwo6CWVJrC"
      },
      "source": [
        "## Example 6: authors and categories analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoOtfxzSPWDD"
      },
      "source": [
        "# List of most prolific authors\n",
        "\n",
        "list_authors = []\n",
        "\n",
        "for new_list in data['author'].str.split('|'):  # Multiple authors are separated by '|'\n",
        "    for author in new_list:\n",
        "        list_authors.append(author)  # List with all the occurrences of authors\n",
        "\n",
        "series_authors = pd.Series(list_authors)  # Create a Series for further manipulation\n",
        "\n",
        "series_authors.value_counts()[:20]  # Top 20 authors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikPXx1HeP7hO"
      },
      "source": [
        "# Create a word cloud with the 'abstracts' of a specific author\n",
        "# Play around! Try a different author\n",
        "\n",
        "author = 'Henri Sterdyniak'\n",
        "\n",
        "list_lines = data[data['author'].str.contains(author)]['abstract'] # Store all the lines of the titles\n",
        "\n",
        "corpus = nlp(' '.join(list_lines))  # Concatenate all the sentences in one string\n",
        "tokens = [w.lower_ for w in corpus if (not w.is_space and not w.is_punct and not w.is_stop)]  # Lowercase removing blanks, punctuation and stopwords\n",
        "corpus = ' '.join(tokens)  # Join again all the words in one string\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=800, background_color='white', min_font_size=10).generate(corpus)  # Create word cloud\n",
        "  \n",
        "plt.figure(figsize=(8, 8), facecolor=None)  # Display the word cloud in an image\n",
        "plt.imshow(wordcloud) \n",
        "plt.axis('off') \n",
        "plt.tight_layout(pad = 0) \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42kUFHF3TA4A"
      },
      "source": [
        "# Create a word cloud for specific JEL cathegories\n",
        "# The JEL classification system was developed for use in the Journal of Economic Literature (JEL)\n",
        "# Itis a standard method of classifying scholarly literature in the field of economics\n",
        "# F: International Economics\n",
        "# I: Health, Education, and Welfare\n",
        "# M: Business Administration and Business Economics | Marketing | Accounting | Personnel Economics\n",
        "# R: Urban, Rural, Regional, Real Estate, and Transportation Economics\n",
        "# Play around! Try a different JEL code\n",
        "\n",
        "jel_code = 'F'\n",
        "\n",
        "list_lines = data[data['jel'] == jel_code]['title']  # Store all the lines of the titles\n",
        "\n",
        "corpus = nlp(' '.join(list_lines))  # Concatenate all the sentences in one string\n",
        "tokens = [w.lower_ for w in corpus if (not w.is_space and not w.is_punct and not w.is_stop)]  # Lowercase removing blanks, punctuation and stopwords\n",
        "corpus = ' '.join(tokens)  # Join again all the words in one string\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=800, background_color='white', min_font_size=10).generate(corpus)  # Create word cloud\n",
        "  \n",
        "plt.figure(figsize=(8, 8), facecolor=None)  # Display the word cloud in an image\n",
        "plt.imshow(wordcloud) \n",
        "plt.axis('off') \n",
        "plt.tight_layout(pad = 0) \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuGoggVzTv4_"
      },
      "source": [
        "# References\n",
        "\n",
        "* [RePEC](http://www.repec.org/)\n",
        "* [JEL Classification System](https://www.aeaweb.org/econlit/jelCodes.php?view=jel)\n"
      ]
    }
  ]
}